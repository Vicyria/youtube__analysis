{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52be4a77",
   "metadata": {
    "id": "0aa06532"
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c527d11",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1697983877963,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "e9c6691b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#keyword to be searched\n",
    "KEYWORD = 'surfing'\n",
    "\n",
    "# max comments to be scraped\n",
    "MAX = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e2dde",
   "metadata": {
    "id": "4e23bf38"
   },
   "source": [
    "# import the required stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea6ab12",
   "metadata": {
    "executionInfo": {
     "elapsed": 11876,
     "status": "ok",
     "timestamp": 1697983889835,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "17e8a34a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import googleapiclient.discovery # youtube api\n",
    "import re\n",
    "from tqdm import tqdm # to get cool progress bar\n",
    "import nltk\n",
    "# VADER\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17425800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1697983889836,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "1fe77d95",
    "outputId": "e8f58fd1-c048-42e9-ac3e-4da2ef155997",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d439a5c6",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697983889836,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "VifdBDomtZtG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for google colab\n",
    "# def get_youtube_object():\n",
    "#     # file = open(\"API_KEY\",'r')                  #opening the file containg my API key\n",
    "\n",
    "#     os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "#     api_service_name = \"youtube\"\n",
    "#     api_version = \"v3\"\n",
    "#     # DEVELOPER_KEY = file.read()                  #reading the API key from the file, did this for security purpose\n",
    "\n",
    "#     youtube = googleapiclient.discovery.build(\n",
    "#         api_service_name, api_version, developerKey = \"AIzaSyB0FQKeiNiUFeH5noF1BIwCeuIdxZewUAc\")\n",
    "#     return youtube\n",
    "# youtube = get_youtube_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3fb820",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697983889836,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "6a074d05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_youtube_object():\n",
    "    file = open(\"API_KEY\",'r')                  #opening the file containg my API key\n",
    "\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    DEVELOPER_KEY = file.read()                  #reading the API key from the file, did this for security purpose\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "    return youtube\n",
    "youtube = get_youtube_object()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6da7a1",
   "metadata": {
    "id": "d2748eeb"
   },
   "source": [
    "## preprocessing fucntions to create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29612ce1",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1697983889836,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "ec402f99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function to chekc if the token we are trying to make is actually a meaningful token\n",
    "def is_token_allowed(token):\n",
    "    return bool(token and str(token).strip() and not token.is_stop and not token.is_punct)\n",
    "\n",
    "\n",
    "\n",
    "# function to preprocess each token at once\n",
    "# lemmatization -- get the base word out of the token, e.g. \"be\" is lemma of \"was\"\n",
    "# strip of extra space or punctuation\n",
    "# convert all to lowercase\n",
    "def preprocess_token(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "# final function which will return a string of created tokens\n",
    "def create_tokens(string):\n",
    "\n",
    "    # load the english language\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # create an object of spacy library\n",
    "    nlp_text = nlp(string)\n",
    "    complete_filtered_tokens = [preprocess_token(token) for token in nlp_text if is_token_allowed(token)]\n",
    "\n",
    "    # remove if it is of length 1, i.e. emoticons and other symbols etc.\n",
    "    complete_filtered_tokens = [x for x in complete_filtered_tokens if len(x)>1]\n",
    "\n",
    "    # return the tokens as one complete string\n",
    "    complete_filtered_tokens = \" \".join(complete_filtered_tokens)\n",
    "    return complete_filtered_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aeb852",
   "metadata": {
    "id": "81dbd2ea"
   },
   "source": [
    "## class video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c31846",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1697983889837,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "61736f99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Video: # video class containing all the data associated with each video id containing comments, metaData, channelData\n",
    "    def __init__(self):\n",
    "        self.comments = 0\n",
    "        self.metaData = 0\n",
    "        self.channelData = 0\n",
    "\n",
    "    # a function to check if all the functions work or not\n",
    "    def do_it_all(self):\n",
    "        # print(\"kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\")\n",
    "        self.process_comments()\n",
    "\n",
    "        self.sentiment_analysis()\n",
    "        # print(self.comments.head())\n",
    "        self.process_metaData()\n",
    "        # print(self.metaData)\n",
    "\n",
    "#         self.create_commentsCloud()\n",
    "        # print(\"kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk\")\n",
    "\n",
    "    # a function to clean and process all the comments and generate tokens from them\n",
    "    def process_comments(self):\n",
    "        try:\n",
    "            # convert 2023-10-21T18:31:54Z to 2023-10-21 18:31:54+00:00\n",
    "            self.comments[\"PublishTime\"] = pd.to_datetime(self.comments[\"PublishTime\"])\n",
    "\n",
    "            # create tokens out of comments\n",
    "            self.comments[\"tokens\"] = self.comments[\"comments\"].apply(create_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error found in Video.process_comments() function: {e}\")\n",
    "    # a function to generate VADER sentiment analysis of the comments -- only to be used after generating tokens using process_comments() function\n",
    "    def sentiment_analysis(self):\n",
    "        try:\n",
    "            SIA=SentimentIntensityAnalyzer()\n",
    "            results={}\n",
    "            for i,row in tqdm(self.comments.iterrows()): # iterate through each row of the comments dataset\n",
    "                text=row[\"comments\"]\n",
    "                myid=row[\"commenter\"]\n",
    "                results[myid]=SIA.polarity_scores(text) # get the polarity scores and store in dictionary\n",
    "            Vaders=pd.DataFrame(results).T\n",
    "            Vaders= Vaders.reset_index().rename(columns={'index': 'commenter'})\n",
    "            self.comments=Vaders.merge(self.comments,how='left') # merge and store back in the original variable\n",
    "\n",
    "            # categorise as positive, negative or neutral based on compound score\n",
    "            self.comments[\"sentiment\"] = pd.cut(self.comments[\"compound\"], bins=[-1,-0.05,0.05,1], labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error found in Video.sentiment_analysis() function: {e}\")\n",
    "\n",
    "    # function to clean the video metadata\n",
    "    def process_metaData(self):\n",
    "        try:\n",
    "            # removing urls from the video description\n",
    "            self.metaData[\"videoDescription\"] = re.sub(r\"(http.+)|(\\n)\",\"\",self.metaData[\"videoDescription\"])\n",
    "\n",
    "            # changing vidLen string into seconds\n",
    "            k = re.search(r\"(?P<hour>\\d+H)?(?P<min>\\d+M)?(?P<sec>\\d+S)\",self.metaData[\"vidLen(sec)\"])\n",
    "            if k:\n",
    "                hours = int(k[\"hour\"][:-1]) if k[\"hour\"] else 0\n",
    "                minutes = int(k[\"min\"][:-1]) if k[\"min\"] else 0\n",
    "                seconds = int(k[\"sec\"][:-1]) if k[\"sec\"] else 0\n",
    "                self.metaData[\"vidLen(sec)\"] = hours * 3600 + minutes * 60 + seconds\n",
    "            else:\n",
    "                print(\"wrong video duration format\")\n",
    "            \n",
    "            \n",
    "            # convert 2023-10-21T18:31:54Z to 2023-10-21 18:31:54+00:00\n",
    "            self.metaData[\"videoPublishTime\"] = pd.to_datetime(self.metaData[\"videoPublishTime\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error found in Video.process_metaData() function: {e}\")\n",
    "\n",
    "\n",
    "    # create a word cloud to see the most used words in the comments\n",
    "    def create_commentsCloud(self, sentiment_type=[\"Positive\", \"Negative\", \"Neutral\"]): #cloud for positive negative or neutral\n",
    "        try:\n",
    "            words = \" \".join(self.comments[self.comments[\"sentiment\"].isin(sentiment_type)][\"tokens\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"error reported in Video.createCloud {e}\")\n",
    "\n",
    "        word_cloud = WordCloud(collocations = False, background_color = 'white',\n",
    "                                width = 4096, height = 2048).generate(words)\n",
    "\n",
    "        plt.imshow(word_cloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c4118",
   "metadata": {
    "id": "bab1e7af"
   },
   "source": [
    "## class Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c48b792f",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1697983889837,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "f98fcc99",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a search class which contains data about top 5 search results\n",
    "class Search:\n",
    "    def __init__(self, keyword, order_by=\"relevance\"):\n",
    "        request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        maxResults=5,\n",
    "        order=order_by,\n",
    "        q=keyword,\n",
    "        type=\"video\"\n",
    "    )\n",
    "        response = request.execute()\n",
    "        self.IDs = {}\n",
    "        for item in response[\"items\"]:\n",
    "            self.IDs[item[\"id\"][\"videoId\"]] = item[\"snippet\"][\"channelId\"] # keys are video ids and values are channel ids\n",
    "\n",
    "        self.videos = {}\n",
    "        for video in self.IDs.keys():\n",
    "            self.videos[video] = Video()\n",
    "        self.get_video_meta_data() # a call is made when an instance of search class is created\n",
    "        self.get_comments() # a call is made when an instance of search class is created\n",
    "        self.get_channel_meta_data()\n",
    "\n",
    "\n",
    "    # a function to get top comments[upto MAX] for all the top search results and store as dataframe\n",
    "    def get_comments(self):\n",
    "        for vID in tqdm(self.IDs.keys(), \"getting comments for each of the top 5 results\"):\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=vID, maxResults=100 #,order='time'\n",
    "            )\n",
    "\n",
    "            response = request.execute()\n",
    "\n",
    "            comments = []\n",
    "            commenter = []\n",
    "            publishedAt = []\n",
    "            totalReplyCount = []\n",
    "            likeCount = []\n",
    "            for item in response['items']:                  #get all top level comments on the first page of the video\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "                comments.append(snippet['textOriginal'])\n",
    "                commenter.append(snippet['authorDisplayName'])\n",
    "                publishedAt.append(snippet['updatedAt'])\n",
    "                likeCount.append(snippet['likeCount'])\n",
    "                totalReplyCount.append(item['snippet']['totalReplyCount'])\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "\n",
    "\n",
    "            while next_page_token is not None:            #getting the comments on the next pages\n",
    "                if len(comments)>MAX:\n",
    "                    break\n",
    "                request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=vID, maxResults=100,\n",
    "                pageToken = next_page_token\n",
    "                )\n",
    "                response = request.execute()\n",
    "\n",
    "                for item in response['items']:\n",
    "                    snippet = item['snippet']['topLevelComment']['snippet']\n",
    "\n",
    "                    comments.append(snippet['textOriginal'])\n",
    "                    commenter.append(snippet['authorDisplayName'])\n",
    "                    publishedAt.append(snippet['updatedAt'])\n",
    "                    likeCount.append(snippet['likeCount'])\n",
    "                    totalReplyCount.append(item['snippet']['totalReplyCount'])\n",
    "\n",
    "                next_page_token = response.get('nextPageToken')\n",
    "\n",
    "            data = {\n",
    "            \"commenter\": commenter,\n",
    "            \"comments\": comments,\n",
    "            \"PublishTime\": publishedAt,\n",
    "            \"totalReplyCount\": totalReplyCount,\n",
    "            \"likeCount\": likeCount\n",
    "        }\n",
    "\n",
    "            self.videos[vID].comments = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "    # a function to get all the meta data about the video and store as a series\n",
    "    def get_video_meta_data(self):\n",
    "\n",
    "        request = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=list(self.IDs.keys())\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in tqdm(response[\"items\"], \"getting metadata for the top five videos\"):\n",
    "            dic = {\n",
    "            \"videoId\": item[\"id\"],\n",
    "            \"videoTitle\": item[\"snippet\"][\"title\"],\n",
    "            \"videoDescription\": item[\"snippet\"][\"description\"],\n",
    "            \"thumbnail\": item[\"snippet\"][\"thumbnails\"][\"medium\"][\"url\"],\n",
    "#             \"channelName\": item[\"snippet\"][\"channelTitle\"],\n",
    "#             \"tags\": item[\"snippet\"][\"tags\"],\n",
    "            \"videoPublishTime\": item[\"snippet\"][\"publishedAt\"],\n",
    "            \"vidLen(sec)\": item[\"contentDetails\"][\"duration\"],\n",
    "            \"viewCount\": item[\"statistics\"][\"viewCount\"],\n",
    "            \"likeCount\": item[\"statistics\"][\"likeCount\"],\n",
    "            \"commentCount\": item[\"statistics\"][\"commentCount\"]\n",
    "            }\n",
    "\n",
    "            self.videos[item[\"id\"]].metaData = pd.Series(dic)\n",
    "#             print(pd.Series(dic))\n",
    "\n",
    "    def get_channel_meta_data(self):\n",
    "        \n",
    "        request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=list(self.IDs.values())\n",
    "        )\n",
    "        \n",
    "        response = request.execute()\n",
    "        try:\n",
    "            for item in tqdm(response[\"items\"], \"getting metadata for the top five video's channels\"):\n",
    "                dic = {\n",
    "                \"channelId\" : item[\"id\"],\n",
    "                \"channelName\": item[\"snippet\"][\"title\"],\n",
    "                \"thumbnail\": item[\"snippet\"][\"thumbnails\"][\"medium\"][\"url\"],\n",
    "#                 \"countryOfOrigin\": item[\"snippet\"][\"country\"],\n",
    "                \"viewCount\": item[\"statistics\"][\"viewCount\"],\n",
    "                \"videoCount\": item[\"statistics\"][\"videoCount\"],\n",
    "                \"subscriberCount\": item[\"statistics\"][\"subscriberCount\"]\n",
    "                }\n",
    "\n",
    "                for key in self.IDs.keys():\n",
    "                    if self.IDs[key] == item[\"id\"]:\n",
    "                        self.videos[key].channelData = pd.Series(dic)\n",
    "        except Exception as e:\n",
    "            print(f\"unable to fetch channel meta data: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56d19087",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1817,
     "status": "ok",
     "timestamp": 1697983891635,
     "user": {
      "displayName": "Ashutosh Goyal",
      "userId": "11242947032501398877"
     },
     "user_tz": -330
    },
    "id": "ed518248",
    "outputId": "037475e8-5ebc-46a9-ec55-2c440a116289",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting metadata for the top five videos: 100%|██████████| 5/5 [00:00<00:00, 1250.09it/s]\n",
      "getting comments for each of the top 5 results: 100%|██████████| 5/5 [00:04<00:00,  1.23it/s]\n",
      "getting metadata for the top five video's channels: 100%|██████████| 5/5 [00:00<00:00, 1667.98it/s]\n"
     ]
    }
   ],
   "source": [
    "search = Search(KEYWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2a36994",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vid1,vid2,vid3,vid4,vid5 = search.videos.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196f72c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 1330.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for vid in search.videos:\n",
    "    search.videos[vid].do_it_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b100bd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "search.videos[vid1].metaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcd9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.videos[vid1].channelData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29976c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "search.videos[vid1].comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a603078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search.videos[vid1].create_commentsCloud([\"Positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install panel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39fff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fae2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b821909",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c63704",
   "metadata": {},
   "source": [
    "# PANELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618dab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "youtube_logo = pn.pane.JPG(\n",
    "    'https://1000logos.net/wp-content/uploads/2021/04/YouTube-logo.png',\n",
    "    link_url='https://www.youtube.com',\n",
    "    width=400\n",
    ")\n",
    "\n",
    "\n",
    "search_keyword_input = pn.widgets.TextInput(name='Search Youtube', placeholder='Enter a text here...')\n",
    "\n",
    "search_by = pn.widgets.AutocompleteInput(\n",
    "    name='Search By', options=[\"date\", \"rating\", \"relevance\", \"viewCount\"],\n",
    "    placeholder='Default: By Relevance', restrict = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a45877d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03346573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting video thumbnails\n",
    "\n",
    "vid_thumb = []\n",
    "\n",
    "for vid in search.videos.values():\n",
    "    thumb = pn.pane.JPG(\n",
    "    vid.metaData[\"thumbnail\"],\n",
    "    link_url=f'https://www.youtube.com/{vid.metaData[\"videoId\"]}',\n",
    "    width=400\n",
    "    )\n",
    "    \n",
    "    vid_thumb.append(thumb)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30900f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an empty dataframe\n",
    "metaData_df = pd.DataFrame(index=[ \"Date Of Publish Of Video\", \"Number Of Likes On Video\", \"Number Of Views On Video\", \"Name Of Channel\", \"Number Of Subscribers Of Channel\", \"Number of Views On Channel\", \"Total Number Of Videos Uploaded\"])\n",
    "\n",
    "# filling values in that dataframe:\n",
    "for video in search.videos.values():\n",
    "    column = pd.Series([video.metaData[\"videoPublishTime\"], video.metaData[\"likeCount\"], video.metaData[\"viewCount\"], video.channelData[\"channelName\"], video.channelData[\"subscriberCount\"], video.channelData[\"viewCount\"], video.channelData[\"videoCount\"]])\n",
    "    metaData_df[video.metaData[\"videoTitle\"]] = column\n",
    "    \n",
    "# naming the columns of that dataframe\\\n",
    "metaData_df.columns.name = \"Name Of Video->\"\n",
    "metaData_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4813c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776599e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c895025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the grid\n",
    "gspec = pn.GridSpec(sizing_mode='stretch_both', max_height=200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Place your components within the grid\n",
    "\n",
    "gspec[0:2, 0:2] = pn.panel(youtube_logo)\n",
    "gspec[0, 2:5] = pn.panel(search_keyword_input)\n",
    "gspec[0, 5] = pn.panel(search_by)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the Panel app with the responsive GridSpec\n",
    "app = pn.panel(gspec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the template with widgets displayed in the sidebar\n",
    "template = pn.template.BootstrapTemplate(\n",
    "    title='BootstrapTemplate',\n",
    "    main = app,\n",
    "    sidebar = []\n",
    ")\n",
    "template.main.append(pn.Row(*vid_thumb))  # *image_row: The * symbol before image_row is known as the \"splat\" operator. It is used to unpack the elements of the image_row list. When we use *image_row, it effectively passes each image pane in the list as a separate argument to the pn.Row function. This is necessary because pn.Row expects its child components to be passed as separate arguments, not as a list.\n",
    "template.main.append(metaData_df)\n",
    "\n",
    "template.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e8b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3a53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98b9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5883be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
